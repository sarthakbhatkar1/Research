DOI,Title,Authors,Journal,Year,Abstract,ResearchRabbitId,Cited By,References,PubMedId
10.1145/3637528.3671573,FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning,"Weirui Kuang, Bingchen Qian, Zitao Li, Daoyuan Chen, Dawei Gao, Xuchen Pan, Yuexiang Xie, Yaliang Li, Bolin Ding, Jingren Zhou",Knowledge Discovery and Data Mining,2024,"Large language models (LLMs) have demonstrated great capabilities in various natural language understanding and generation tasks. These pre-trained LLMs can be further improved for specific downstream tasks by fine-tuning. However, the adoption of LLM in real-world applications can be hindered by privacy concerns and the resource-intensive nature of model training and fine-tuning. When multiple entities have similar interested tasks but cannot directly share their local data due to privacy regulations, federated learning (FL) is a mainstream solution to leverage the data of different entities. Besides avoiding direct data sharing, FL can also achieve rigorous data privacy protection, model intelligent property protection, and model customization via composition with different techniques. Despite the aforementioned advantages of FL, fine-tuning LLMs in FL settings still lacks adequate support from the existing frameworks and, therefore, faces challenges in optimizing the consumption of significant communication and computational resources, preparing various data for different tasks, and satisfying diverse information protection demands. In this paper, we discuss these challenges and introduce our package FederatedScope-LLM (FS-LLM) as a main contribution, which consists: (1) We build a complete end-to-end benchmarking pipeline under real-world scenarios, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation; (2) We provide comprehensive and off-the-shelf federated parameter-efficient fine-tuning (PEFT) algorithm implementations and versatile programming interfaces for future extension, enhancing the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model; (3) We adopt several accelerating and resource-efficient operators, and provide flexible pluggable sub-routines for interdisciplinary study. We conduct extensive and reproducible experiments to show the effectiveness of FS-LLM and benchmark advanced LLMs with PEFT algorithms in FL. We release FS-LLM at https://github.com/alibaba/FederatedScope/tree/llm.",2b88bcaf-a295-4fd4-868f-dea4e59ff8fe,252,158,(missing PubMedId)
10.1109/TPAMI.2024.3418862,"Federated Learning for Generalization, Robustness, Fairness: A Survey and Benchmark","Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He Li, Bo Du, Qiang Yang",IEEE Transactions on Pattern Analysis and Machine Intelligence,2023,"Federated learning has emerged as a promising paradigm for privacy-preserving collaboration among different parties. Recently, with the popularity of federated learning, an influx of approaches have delivered towards different realistic challenges. In this survey, we provide a systematic overview of the important and recent developments of research on federated learning. First, we introduce the study history and terminology definition of this area. Then, we comprehensively review three basic lines of research: generalization, robustness, and fairness, by introducing their respective background concepts, task settings, and main challenges. We also offer a detailed overview of representative literature on both methods and datasets. We further benchmark the reviewed methods on several well-known datasets. Finally, we point out several open issues in this field and suggest opportunities for further research.",decdd679-b320-44a0-89f3-94401552999a,194,411,https://pubmed.ncbi.nlm.nih.gov/38917282
10.48550/ARXIV.2411.03286,DiT4Edit: Diffusion Transformer for Image Editing,"Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, Zeyu Wang",arXiv.org,2024,"Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patch merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. 
Extensive experiments demonstrate the strong performance of DiT4Edit in various editing scenarios, highlighting the potential of diffusion transformers for image editing.",dc42c347-c790-4e02-af2f-0032ad02edd2,70,73,(missing PubMedId)
10.1109/TKDE.2024.3382002,A Survey for Federated Learning Evaluations: Goals and Measures,"Di Chai, Leye Wang, Liu-Nan Yang, Junxue Zhang, Kai Chen, Qiang Yang",IEEE Transactions on Knowledge and Data Engineering,2023,"Evaluation is a systematic approach to assessing how well a system achieves its intended purpose. Federated learning (FL) is a novel paradigm for privacy-preserving machine learning that allows multiple parties to collaboratively train models without sharing sensitive data. However, evaluating FL is challenging due to its interdisciplinary nature and diverse goals, such as utility, efficiency, and security. In this survey, we first review the major evaluation goals adopted in the existing studies and then explore the evaluation metrics used for each goal. We also introduce FedEval, an open-source platform that provides a standardized and comprehensive evaluation framework for FL algorithms in terms of their utility, efficiency, and security. Finally, we discuss several challenges and future research directions for FL evaluation.",dc61ee1f-fa7d-4e42-8c27-ebb7e4d792c7,35,189,(missing PubMedId)
10.48550/ARXIV.2411.10714,FlexFL: Flexible and Effective Fault Localization with Open-Source Large Language Models,"Chunfang Xu, Zhongxin Liu, Xiaoxue Ren, Gehao Zhang, Ming Liang, David Lo",arXiv.org,2024,"Due to the impressive code comprehension ability of Large Language Models (LLMs), a few studies have proposed to leverage LLMs to locate bugs, i.e., LLM-based FL, and demonstrated promising performance. However, first, these methods are limited in flexibility. They rely on bug-triggering test cases to perform FL and cannot make use of other available bug-related information, e.g., bug reports. Second, they are built upon proprietary LLMs, which are, although powerful, confronted with risks in data privacy. To address these limitations, we propose a novel LLM-based FL framework named FlexFL, which can flexibly leverage different types of bug-related information and effectively work with open-source LLMs. FlexFL is composed of two stages. In the first stage, FlexFL reduces the search space of buggy code using state-of-the-art FL techniques of different families and provides a candidate list of bug-related methods. In the second stage, FlexFL leverages LLMs to delve deeper to double-check the code snippets of methods suggested by the first stage and refine fault localization results. In each stage, FlexFL constructs agents based on open-source LLMs, which share the same pipeline that does not postulate any type of bug-related information and can interact with function calls without the out-of-the-box capability. Extensive experimental results on Defects4J demonstrate that FlexFL outperforms the baselines and can work with different open-source LLMs. Specifically, FlexFL with a lightweight open-source LLM Llama3-8B can locate 42 and 63 more bugs than two state-of-the-art LLM-based FL approaches AutoFL and AgentFL that both use GPT-3.5.",32d3f700-99a7-402b-9e72-ac31ad58097e,12,82,(missing PubMedId)
10.1038/S41578-025-00772-8,Large language models for reticular chemistry,"Zhiling Zheng,  Nakul Rampal,  Théo Jaffrelot Inizan,  C. Borgs,  J. Chayes,  O. Yaghi",Nature Reviews Materials,2025,(missing abstract),1df5e1e9-ce76-414d-9856-81130542cac8,71,170,(missing PubMedId)
10.1016/J.NEUNET.2025.107222,TF-BERT: Tensor-based fusion BERT for multimodal sentiment analysis,"Jingming Hou,  Nazlia Omar,  Sabrina Tiun,  Saidah Saad,  Qian He",Neural Networks,2025,(missing abstract),326329f9-3c77-4389-b9ba-fbdaf4561d52,14,75,https://pubmed.ncbi.nlm.nih.gov/39938440
10.1007/S10462-025-11162-5,BERT applications in natural language processing: a review,"Nadia Mushtaq Gardazi,  Ali Daud,  Muhammad Kamran Malik,  Amal Bukhari,  Tariq Alsahfi,  Bader Alshemaimri",Artificial Intelligence Review,2025,(missing abstract),d957d014-f33f-4755-aa8f-3119880a4b05,73,305,(missing PubMedId)
10.1109/TSE.2025.3553363,FlexFL: Flexible and Effective Fault Localization with Open-Source Large Language Models,"Chunfang Xu,  Zhongxin Liu,  Xiaoxue Ren,  Gehao Zhang,  Ming Liang,  David Lo",IEEE Transactions on Software Engineering,2025,"Fault localization (FL) targets identifying bug locations within a software system, which can enhance debugging efficiency and improve software quality. Due to the impressive code comprehension ability of Large Language Models (LLMs), a few studies have proposed to leverage LLMs to locate bugs, i.e., LLM-based FL, and demonstrated promising performance. However, first, these methods are limited in flexibility. They rely on bug-triggering test cases to perform FL and cannot make use of other available bug-related information, e.g., bug reports. Second, they are built upon proprietary LLMs, which are, although powerful, confronted with risks in data privacy. To address these limitations, we propose a novel LLM-based FL framework named FlexFL, which can flexibly leverage different types of bug-related information and effectively work with open-source LLMs. FlexFL is composed of two stages. In the first stage, FlexFL reduces the search space of buggy code using state-of-the-art FL techniques of different families and provides a candidate list of bug-related methods. In the second stage, FlexFL leverages LLMs to delve deeper to double-check the code snippets of methods suggested by the first stage and refine fault localization results. In each stage, FlexFL constructs agents based on open-source LLMs, which share the same pipeline that does not postulate any type of bug-related information and can interact with function calls without the out-of-the-box capability. Extensive experimental results on Defects4J demonstrate that FlexFL outperforms the baselines and can work with different open-source LLMs. Specifically, FlexFL with a lightweight open-source LLM Llama3-8B can locate 42 and 63 more bugs than two state-of-the-art LLM-based FL approaches AutoFL and AgentFL that both use GPT-3.5. In addition, FlexFL can localize 93 bugs that cannot be localized by non-LLM-based FL techniques at the top 1. Furthermore, to mitigate potential data contamination, we conduct experiments on a dataset which Llama3-8B has not seen before, and the evaluation results show that FlexFL can also achieve good performance.",a2005c39-8af9-469b-8414-ce5063a83061,12,82,(missing PubMedId)
10.1109/MCOM.001.2300550,Large Language Models Empowered Autonomous Edge AI for Connected Intelligence,"Yifei Shen,  Jiawei Shao,  Xinjie Zhang,  Zehong Lin, Hao Pan, Dongsheng Li, Jun Zhang,  Khaled B. Letaief",IEEE Communications Magazine,2024,(missing abstract),7c9090e8-b5e5-484b-9d2d-b15193dd8394,160,34,(missing PubMedId)
10.1145/3625558,Heterogeneous Federated Learning: State-of-the-art and Research Challenges,"Mang Ye, Xiuwen Fang, Bo Du, Pong C. Yuen, Dacheng Tao",ACM Computing Surveys,2023,"Federated learning (FL) has drawn increasing attention owing to its potential use in large-scale industrial applications. Existing FL works mainly focus on model homogeneous settings. However, practical FL typically faces the heterogeneity of data distributions, model architectures, network environments, and hardware devices among participant clients. Heterogeneous Federated Learning (HFL) is much more challenging, and corresponding solutions are diverse and complex. Therefore, a systematic survey on this topic about the research challenges and state-of-the-art is essential. In this survey, we firstly summarize the various research challenges in HFL from five aspects: statistical heterogeneity, model heterogeneity, communication heterogeneity, device heterogeneity, and additional challenges. In addition, recent advances in HFL are reviewed and a new taxonomy of existing HFL methods is proposed with an in-depth analysis of their pros and cons. We classify existing methods from three different levels according to the HFL procedure: data-level, model-level, and server-level. Finally, several critical and promising future research directions in HFL are discussed, which may facilitate further developments in this field. A periodically updated collection on HFL is available at https://github.com/marswhu/HFL_Survey.",18917d11-cd07-461a-90c1-5f4f39419f91,499,436,(missing PubMedId)
10.48550/ARXIV.2211.12814,"Vertical Federated Learning: Concepts, Advances, and Challenges","Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yu He, Xiaozhou Ye, Ye Ouyang, Yaqin Zhang, Qiang Yang",arXiv.org,2022,"Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.",050c8116-3d94-422b-942c-89e71886c7dd,292,286,(missing PubMedId)
10.48550/ARXIV.2310.15080,Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization,"Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou",arXiv.org,2023,"Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8\% in terms of accuracy) and efficiency (up to 97.59\% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",f3b4e18c-af92-4486-8032-8cbac14daa6d,105,137,(missing PubMedId)
10.1109/TKDE.2024.3352628,"Vertical Federated Learning: Concepts, Advances, and Challenges","Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, Qiang Yang",IEEE Transactions on Knowledge and Data Engineering,2024,"Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.",f4ee49f8-fa3b-4900-8e69-8437dd341630,341,283,(missing PubMedId)
10.48550/ARXIV.2406.04845,FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models,"Rui Ye, Rui Ge, Xinyu Zhu, Jingyi Chai, Yaxin Du, Yang Liu, Yanfeng Wang, Siheng Chen",arXiv.org,2024,"Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM). Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy. However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios. Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community. FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747. Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios. Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration). We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons. Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.",2fd57730-2526-4e06-abbf-aa532b853551,28,121,(missing PubMedId)
10.1109/TKDE.2024.3352628,"Vertical Federated Learning: Concepts, Advances, and Challenges","Yang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yu He, Xiaozhou Ye, Ye Ouyang, Yaqin Zhang, Qiang Yang",IEEE Transactions on Knowledge and Data Engineering,2022,"Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.",b136a7a9-cf39-4e60-8db7-5dc5025831aa,332,283,(missing PubMedId)
10.48550/ARXIV.2408.13010,A Web-Based Solution for Federated Learning with LLM-Based Automation,"Chamith Mawela,  Chaouki Ben Issaid,  Mehdi Bennis",arXiv.org,2024,"Federated learning (FL) offers a promising approach for collaborative machine learning (ML) across distributed devices. However, its adoption is hindered by the complexity of building reliable communication architectures and the need for expertise in both ML and network programming. This article presents a comprehensive solution that simplifies the orchestration of FL tasks while integrating intent-based automation. A user-friendly web application is developed supporting the federated averaging (FedAvg) algorithm, enabling users to configure parameters through an intuitive interface. The backend solution efficiently manages communication between the parameter server and edge nodes. Model compression and scheduling algorithms are implemented to optimize FL performance. Additionally, intent-based automation in FL is explored using a fine-tuned Language Model (LLM) trained on a tailored dataset, enabling users to perform FL tasks through high-level prompts. It is shown that the LLM-based automated solution achieves comparable test accuracy to the standard web-based solution while reducing transferred bytes by up to 64% and CPU time by up to 46% for FL tasks. Furthermore, neural architecture search (NAS) and hyperparameter optimization (HPO) are leveraged using the LLM to enhance performance, resulting in a 10%–20% improvement in test accuracy for the conducted FL tasks.",1f1c74ea-ac22-4dc1-a082-b5a6633245bd,6,68,(missing PubMedId)
10.1145/3637528.3671865,On the Convergence of Zeroth-Order Federated Tuning for Large Language Models,"Zhenqing Ling, Daoyuan Chen, Liu-Yi Yao, Yaliang Li, Ying Shen",Knowledge Discovery and Data Mining,2024,"The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on clients with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we term as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as FedAvg but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs, thereby stimulating further advancements and research in this area.",330acc11-c985-4f43-91c3-cb2c529ee56b,33,88,(missing PubMedId)
10.48550/ARXIV.2504.16032,LLMs meet Federated Learning for Scalable and Secure IoT Management,"Yazan Otoum, Arghavan Asad, Amiya Nayak",arXiv.org,2025,"The rapid expansion of IoT ecosystems introduces severe challenges in scalability, security, and real-time decision-making. Traditional centralized architectures struggle with latency, privacy concerns, and excessive resource consumption, making them unsuitable for modern large-scale IoT deployments. This paper presents a novel Federated Learning-driven Large Language Model (FL-LLM) framework, designed to enhance IoT system intelligence while ensuring data privacy and computational efficiency. The framework integrates Generative IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS), dynamically optimizing model updates based on real-time network conditions. By leveraging a hybrid edge-cloud processing architecture, our approach balances intelligence, scalability, and security in distributed IoT environments. Evaluations on the IoT-23 dataset demonstrate that our framework improves model accuracy, reduces response latency, and enhances energy efficiency, outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings highlight the potential of integrating LLM-powered federated learning into large-scale IoT ecosystems, paving the way for more secure, scalable, and adaptive IoT management solutions.",1ce7c306-08aa-49cb-b0d8-7524bb3696cf,6,19,(missing PubMedId)
10.48550/ARXIV.2310.10049,FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models,"Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang",arXiv.org,2023,"Large Language Models (LLMs), such as ChatGPT, LLaMA, GLM, and PaLM, have exhibited remarkable performances across various tasks in recent years. However, LLMs face two main challenges in real-world applications. One challenge is that training LLMs consumes vast computing resources, preventing LLMs from being adopted by small and medium-sized enterprises with limited computing resources. Another is that training LLM requires a large amount of high-quality data, which are often scattered among enterprises. To address these challenges, we propose FATE-LLM, an industrial-grade federated learning framework for large language models. FATE-LLM (1) facilitates federated learning for large language models (coined FedLLM); (2) promotes efficient training of FedLLM using parameter-efficient fine-tuning methods; (3) protects the intellectual property of LLMs; (4) preserves data privacy during training and inference through privacy-preserving mechanisms. We release the code of FATE-LLM at https://github.com/FederatedAI/FATE-LLM to facilitate the research of FedLLM and enable a broad range of industrial applications.",8e974b90-26d9-42ea-b865-e85ddfaec2f7,109,69,(missing PubMedId)
